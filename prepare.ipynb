{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4119761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598cb7e5",
   "metadata": {},
   "source": [
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "    * Lowercase everything\n",
    "    * Normalize unicode characters\n",
    "    * Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a80c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(article):\n",
    "    \"\"\" lowercases, normalizes, and destroys special characters of an article (string) \"\"\"\n",
    "    # lowercase\n",
    "    article = article.lower()\n",
    "    # normalize\n",
    "    article = unicodedata.normalize('NFKD', article).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # remove special characters\n",
    "    article = re.sub(r\"[^a-z0-9'\\s]\", \"\", article)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccfba6d",
   "metadata": {},
   "source": [
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db1f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(article):\n",
    "    \"\"\" tokenize a basic_clean-ed article (string) \"\"\"\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer() # create tokenizer\n",
    "    article = tokenizer.tokenize(article, return_str = True) # tokenize\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b74d94c",
   "metadata": {},
   "source": [
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8f3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(article):\n",
    "    \"\"\" stem all words in an article (string) \"\"\"\n",
    "    ps = nltk.porter.PorterStemmer() # create stemmer\n",
    "    stems = [ps.stem(word) for word in article.split()] # list comprehension of stems\n",
    "    article_stemmed = ' '.join(stems) # re-join list as article\n",
    "    \n",
    "    return article_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa00fe",
   "metadata": {},
   "source": [
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29b3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(article):\n",
    "    \"\"\" lemma all words in an article (string) \"\"\"\n",
    "    nltk.download('wordnet') # get current lemma list\n",
    "    wnl = nltk.stem.WordNetLemmatizer() # create lemmatizer\n",
    "    lemmas = [wnl.lemmatize(word) for word in article.split()] # list comp of lemmas\n",
    "    article_lemmatized = ' '.join(lemmas) # re-join list as article\n",
    "    \n",
    "    return article_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c485ba",
   "metadata": {},
   "source": [
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "    * This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9029f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(article):\n",
    "    \"\"\" remove stopwords from an article (string) \"\"\"\n",
    "    stopword_list = stopwords.words('english') # get default stopword list\n",
    "    words = article_lemmatized.split() # split for stopword removal\n",
    "    filtered_words = [word for word in words if word not in stopword_list] # ignore stopwords\n",
    "    article_without_stopwords = ' '.join(filtered_words) # re-join list to article\n",
    "    \n",
    "    return article_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7900a6",
   "metadata": {},
   "source": [
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c8884a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Refer friends &amp; get a chance to win Bitcoin wo...</td>\n",
       "      <td>11:39 am</td>\n",
       "      <td>Business</td>\n",
       "      <td>CoinSwitch Kuber has launched 'CSK Referral Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making quality medicines super affordable with...</td>\n",
       "      <td>10:00 am</td>\n",
       "      <td>Business</td>\n",
       "      <td>Apollo 24|7 is offering flat 25% discount on f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China's new COVID-19 outbreak wipes $4 billion...</td>\n",
       "      <td>03:51 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>China's top hot pot chain has lost $4 billion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shiba Inu jumps 40% to record high after anony...</td>\n",
       "      <td>02:46 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>Meme-based cryptocurrency Shiba Inu (SHIB) jum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow, 13 years ago: Musk on old video from when...</td>\n",
       "      <td>03:46 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>Tesla CEO and the world's richest person Elon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Was heartbreaking to see SRK going to jail to ...</td>\n",
       "      <td>06:31 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Adhyayan Summan spoke about his tweet wherein ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Went for drive, forgot Mehr when she was 40 da...</td>\n",
       "      <td>06:33 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Actress Neha Dhupia has revealed that she and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>'Dirty Little Billy', 'Star Trek' actor Richar...</td>\n",
       "      <td>07:58 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Veteran Hollywood actor Richard Evans passed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sequel to Shahid's debut film 'Ishq Vishk' in ...</td>\n",
       "      <td>08:37 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>A sequel to the 2003 film 'Ishq Vishk', which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>We fell in love &amp; have been inseparable ever s...</td>\n",
       "      <td>03:04 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Speaking about her relationship with her husba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline publish_time  \\\n",
       "0   Refer friends & get a chance to win Bitcoin wo...     11:39 am   \n",
       "1   Making quality medicines super affordable with...     10:00 am   \n",
       "2   China's new COVID-19 outbreak wipes $4 billion...     03:51 pm   \n",
       "3   Shiba Inu jumps 40% to record high after anony...     02:46 pm   \n",
       "4   Wow, 13 years ago: Musk on old video from when...     03:46 pm   \n",
       "..                                                ...          ...   \n",
       "20  Was heartbreaking to see SRK going to jail to ...     06:31 pm   \n",
       "21  Went for drive, forgot Mehr when she was 40 da...     06:33 pm   \n",
       "22  'Dirty Little Billy', 'Star Trek' actor Richar...     07:58 pm   \n",
       "23  Sequel to Shahid's debut film 'Ishq Vishk' in ...     08:37 pm   \n",
       "24  We fell in love & have been inseparable ever s...     03:04 pm   \n",
       "\n",
       "         category                                            content  \n",
       "0        Business  CoinSwitch Kuber has launched 'CSK Referral Le...  \n",
       "1        Business  Apollo 24|7 is offering flat 25% discount on f...  \n",
       "2        Business  China's top hot pot chain has lost $4 billion ...  \n",
       "3        Business  Meme-based cryptocurrency Shiba Inu (SHIB) jum...  \n",
       "4        Business  Tesla CEO and the world's richest person Elon ...  \n",
       "..            ...                                                ...  \n",
       "20  Entertainment  Adhyayan Summan spoke about his tweet wherein ...  \n",
       "21  Entertainment  Actress Neha Dhupia has revealed that she and ...  \n",
       "22  Entertainment  Veteran Hollywood actor Richard Evans passed a...  \n",
       "23  Entertainment  A sequel to the 2003 film 'Ishq Vishk', which ...  \n",
       "24  Entertainment  Speaking about her relationship with her husba...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = acquire.get_news()\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae8bfa",
   "metadata": {},
   "source": [
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af318d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>Sep 30, 2018</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>Oct 31, 2018</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust Data Scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>Oct 17, 2018</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>By Dimitri Antoniou A week ago, Codeup launche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>Aug 14, 2018</td>\n",
       "      <td>Tips for Prospective Students</td>\n",
       "      <td>The third bi-annual San Antonio Tech Job Fair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>Aug 14, 2018</td>\n",
       "      <td>Codeup News</td>\n",
       "      <td>In recent news, DevBootcamp and The Iron Yar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title          date  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  Sep 30, 2018   \n",
       "1                                 Data Science Myths  Oct 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...  Oct 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair  Aug 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  Aug 14, 2018   \n",
       "\n",
       "                        category  \\\n",
       "0                   Data Science   \n",
       "1                   Data Science   \n",
       "2                   Data Science   \n",
       "3  Tips for Prospective Students   \n",
       "4                    Codeup News   \n",
       "\n",
       "                                             content  \n",
       "0  The rumors are true! The time has arrived. Cod...  \n",
       "1  By Dimitri Antoniou and Maggie Giust Data Scie...  \n",
       "2  By Dimitri Antoniou A week ago, Codeup launche...  \n",
       "3  The third bi-annual San Antonio Tech Job Fair ...  \n",
       "4    In recent news, DevBootcamp and The Iron Yar...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = acquire.get_blogs()\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2ac5c",
   "metadata": {},
   "source": [
    "8. For each dataframe, produce the following columns:\n",
    "    * title to hold the title\n",
    "    * original to hold the original article/post content\n",
    "    * clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "    * stemmed to hold the stemmed version of the cleaned data.\n",
    "    * lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d429f3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Refer friends &amp; get a chance to win Bitcoin wo...</td>\n",
       "      <td>11:39 am</td>\n",
       "      <td>Business</td>\n",
       "      <td>CoinSwitch Kuber has launched 'CSK Referral Le...</td>\n",
       "      <td>coinswitch kuber has launched ' csk referral l...</td>\n",
       "      <td>coinswitch kuber ha launch ' csk referr leagu ...</td>\n",
       "      <td>coinswitch kuber ha launched ' csk referral le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making quality medicines super affordable with...</td>\n",
       "      <td>10:00 am</td>\n",
       "      <td>Business</td>\n",
       "      <td>Apollo 24|7 is offering flat 25% discount on f...</td>\n",
       "      <td>apollo 247 is offering flat 25 discount on fir...</td>\n",
       "      <td>apollo 247 is offer flat 25 discount on first ...</td>\n",
       "      <td>apollo 247 is offering flat 25 discount on fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China's new COVID-19 outbreak wipes $4 billion...</td>\n",
       "      <td>03:51 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>China's top hot pot chain has lost $4 billion ...</td>\n",
       "      <td>china ' s top hot pot chain has lost 4 billion...</td>\n",
       "      <td>china ' s top hot pot chain ha lost 4 billion ...</td>\n",
       "      <td>china ' s top hot pot chain ha lost 4 billion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shiba Inu jumps 40% to record high after anony...</td>\n",
       "      <td>02:46 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>Meme-based cryptocurrency Shiba Inu (SHIB) jum...</td>\n",
       "      <td>memebased cryptocurrency shiba inu shib jumped...</td>\n",
       "      <td>memebas cryptocurr shiba inu shib jump over 40...</td>\n",
       "      <td>memebased cryptocurrency shiba inu shib jumped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow, 13 years ago: Musk on old video from when...</td>\n",
       "      <td>03:46 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>Tesla CEO and the world's richest person Elon ...</td>\n",
       "      <td>tesla ceo and the world ' s richest person elo...</td>\n",
       "      <td>tesla ceo and the world ' s richest person elo...</td>\n",
       "      <td>tesla ceo and the world ' s richest person elo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Was heartbreaking to see SRK going to jail to ...</td>\n",
       "      <td>06:31 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Adhyayan Summan spoke about his tweet wherein ...</td>\n",
       "      <td>adhyayan summan spoke about his tweet wherein ...</td>\n",
       "      <td>adhyayan summan spoke about hi tweet wherein h...</td>\n",
       "      <td>adhyayan summan spoke about his tweet wherein ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Went for drive, forgot Mehr when she was 40 da...</td>\n",
       "      <td>06:33 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Actress Neha Dhupia has revealed that she and ...</td>\n",
       "      <td>actress neha dhupia has revealed that she and ...</td>\n",
       "      <td>actress neha dhupia ha reveal that she and hus...</td>\n",
       "      <td>actress neha dhupia ha revealed that she and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>'Dirty Little Billy', 'Star Trek' actor Richar...</td>\n",
       "      <td>07:58 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Veteran Hollywood actor Richard Evans passed a...</td>\n",
       "      <td>veteran hollywood actor richard evans passed a...</td>\n",
       "      <td>veteran hollywood actor richard evan pass away...</td>\n",
       "      <td>veteran hollywood actor richard evans passed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sequel to Shahid's debut film 'Ishq Vishk' in ...</td>\n",
       "      <td>08:37 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>A sequel to the 2003 film 'Ishq Vishk', which ...</td>\n",
       "      <td>a sequel to the 2003 film ' ishq vishk ' which...</td>\n",
       "      <td>a sequel to the 2003 film ' ishq vishk ' which...</td>\n",
       "      <td>a sequel to the 2003 film ' ishq vishk ' which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>We fell in love &amp; have been inseparable ever s...</td>\n",
       "      <td>03:04 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Speaking about her relationship with her husba...</td>\n",
       "      <td>speaking about her relationship with her husba...</td>\n",
       "      <td>speak about her relationship with her husband ...</td>\n",
       "      <td>speaking about her relationship with her husba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline publish_time  \\\n",
       "0   Refer friends & get a chance to win Bitcoin wo...     11:39 am   \n",
       "1   Making quality medicines super affordable with...     10:00 am   \n",
       "2   China's new COVID-19 outbreak wipes $4 billion...     03:51 pm   \n",
       "3   Shiba Inu jumps 40% to record high after anony...     02:46 pm   \n",
       "4   Wow, 13 years ago: Musk on old video from when...     03:46 pm   \n",
       "..                                                ...          ...   \n",
       "20  Was heartbreaking to see SRK going to jail to ...     06:31 pm   \n",
       "21  Went for drive, forgot Mehr when she was 40 da...     06:33 pm   \n",
       "22  'Dirty Little Billy', 'Star Trek' actor Richar...     07:58 pm   \n",
       "23  Sequel to Shahid's debut film 'Ishq Vishk' in ...     08:37 pm   \n",
       "24  We fell in love & have been inseparable ever s...     03:04 pm   \n",
       "\n",
       "         category                                            content  \\\n",
       "0        Business  CoinSwitch Kuber has launched 'CSK Referral Le...   \n",
       "1        Business  Apollo 24|7 is offering flat 25% discount on f...   \n",
       "2        Business  China's top hot pot chain has lost $4 billion ...   \n",
       "3        Business  Meme-based cryptocurrency Shiba Inu (SHIB) jum...   \n",
       "4        Business  Tesla CEO and the world's richest person Elon ...   \n",
       "..            ...                                                ...   \n",
       "20  Entertainment  Adhyayan Summan spoke about his tweet wherein ...   \n",
       "21  Entertainment  Actress Neha Dhupia has revealed that she and ...   \n",
       "22  Entertainment  Veteran Hollywood actor Richard Evans passed a...   \n",
       "23  Entertainment  A sequel to the 2003 film 'Ishq Vishk', which ...   \n",
       "24  Entertainment  Speaking about her relationship with her husba...   \n",
       "\n",
       "                                                clean  \\\n",
       "0   coinswitch kuber has launched ' csk referral l...   \n",
       "1   apollo 247 is offering flat 25 discount on fir...   \n",
       "2   china ' s top hot pot chain has lost 4 billion...   \n",
       "3   memebased cryptocurrency shiba inu shib jumped...   \n",
       "4   tesla ceo and the world ' s richest person elo...   \n",
       "..                                                ...   \n",
       "20  adhyayan summan spoke about his tweet wherein ...   \n",
       "21  actress neha dhupia has revealed that she and ...   \n",
       "22  veteran hollywood actor richard evans passed a...   \n",
       "23  a sequel to the 2003 film ' ishq vishk ' which...   \n",
       "24  speaking about her relationship with her husba...   \n",
       "\n",
       "                                              stemmed  \\\n",
       "0   coinswitch kuber ha launch ' csk referr leagu ...   \n",
       "1   apollo 247 is offer flat 25 discount on first ...   \n",
       "2   china ' s top hot pot chain ha lost 4 billion ...   \n",
       "3   memebas cryptocurr shiba inu shib jump over 40...   \n",
       "4   tesla ceo and the world ' s richest person elo...   \n",
       "..                                                ...   \n",
       "20  adhyayan summan spoke about hi tweet wherein h...   \n",
       "21  actress neha dhupia ha reveal that she and hus...   \n",
       "22  veteran hollywood actor richard evan pass away...   \n",
       "23  a sequel to the 2003 film ' ishq vishk ' which...   \n",
       "24  speak about her relationship with her husband ...   \n",
       "\n",
       "                                           lemmatized  \n",
       "0   coinswitch kuber ha launched ' csk referral le...  \n",
       "1   apollo 247 is offering flat 25 discount on fir...  \n",
       "2   china ' s top hot pot chain ha lost 4 billion ...  \n",
       "3   memebased cryptocurrency shiba inu shib jumped...  \n",
       "4   tesla ceo and the world ' s richest person elo...  \n",
       "..                                                ...  \n",
       "20  adhyayan summan spoke about his tweet wherein ...  \n",
       "21  actress neha dhupia ha revealed that she and h...  \n",
       "22  veteran hollywood actor richard evans passed a...  \n",
       "23  a sequel to the 2003 film ' ishq vishk ' which...  \n",
       "24  speaking about her relationship with her husba...  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean'] = news_df['content'].apply(basic_clean).apply(tokenize)\n",
    "news_df['stemmed'] = news_df['clean'].apply(stem)\n",
    "news_df['lemmatized'] = news_df['clean'].apply(lemmatize)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbddc8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>Sep 30, 2018</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "      <td>the rumors are true the time has arrived codeu...</td>\n",
       "      <td>the rumor are true the time ha arriv codeup ha...</td>\n",
       "      <td>the rumor are true the time ha arrived codeup ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>Oct 31, 2018</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust Data Scie...</td>\n",
       "      <td>by dimitri antoniou and maggie giust data scie...</td>\n",
       "      <td>by dimitri antoni and maggi giust data scienc ...</td>\n",
       "      <td>by dimitri antoniou and maggie giust data scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>Oct 17, 2018</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>By Dimitri Antoniou A week ago, Codeup launche...</td>\n",
       "      <td>by dimitri antoniou a week ago codeup launched...</td>\n",
       "      <td>by dimitri antoni a week ago codeup launch our...</td>\n",
       "      <td>by dimitri antoniou a week ago codeup launched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>Aug 14, 2018</td>\n",
       "      <td>Tips for Prospective Students</td>\n",
       "      <td>The third bi-annual San Antonio Tech Job Fair ...</td>\n",
       "      <td>the third biannual san antonio tech job fair i...</td>\n",
       "      <td>the third biannual san antonio tech job fair i...</td>\n",
       "      <td>the third biannual san antonio tech job fair i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>Aug 14, 2018</td>\n",
       "      <td>Codeup News</td>\n",
       "      <td>In recent news, DevBootcamp and The Iron Yar...</td>\n",
       "      <td>in recent news devbootcamp and the iron yard a...</td>\n",
       "      <td>in recent news devbootcamp and the iron yard a...</td>\n",
       "      <td>in recent news devbootcamp and the iron yard a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title          date  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  Sep 30, 2018   \n",
       "1                                 Data Science Myths  Oct 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...  Oct 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair  Aug 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  Aug 14, 2018   \n",
       "\n",
       "                        category  \\\n",
       "0                   Data Science   \n",
       "1                   Data Science   \n",
       "2                   Data Science   \n",
       "3  Tips for Prospective Students   \n",
       "4                    Codeup News   \n",
       "\n",
       "                                             content  \\\n",
       "0  The rumors are true! The time has arrived. Cod...   \n",
       "1  By Dimitri Antoniou and Maggie Giust Data Scie...   \n",
       "2  By Dimitri Antoniou A week ago, Codeup launche...   \n",
       "3  The third bi-annual San Antonio Tech Job Fair ...   \n",
       "4    In recent news, DevBootcamp and The Iron Yar...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  the rumors are true the time has arrived codeu...   \n",
       "1  by dimitri antoniou and maggie giust data scie...   \n",
       "2  by dimitri antoniou a week ago codeup launched...   \n",
       "3  the third biannual san antonio tech job fair i...   \n",
       "4  in recent news devbootcamp and the iron yard a...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  the rumor are true the time ha arriv codeup ha...   \n",
       "1  by dimitri antoni and maggi giust data scienc ...   \n",
       "2  by dimitri antoni a week ago codeup launch our...   \n",
       "3  the third biannual san antonio tech job fair i...   \n",
       "4  in recent news devbootcamp and the iron yard a...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  the rumor are true the time ha arrived codeup ...  \n",
       "1  by dimitri antoniou and maggie giust data scie...  \n",
       "2  by dimitri antoniou a week ago codeup launched...  \n",
       "3  the third biannual san antonio tech job fair i...  \n",
       "4  in recent news devbootcamp and the iron yard a...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df['clean'] = codeup_df['content'].apply(basic_clean).apply(tokenize)\n",
    "codeup_df['stemmed'] = codeup_df['clean'].apply(stem)\n",
    "codeup_df['lemmatized'] = codeup_df['clean'].apply(lemmatize)\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd2c16",
   "metadata": {},
   "source": [
    "9. Ask yourself:\n",
    "    * If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "    * If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "    * If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486bf57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
