{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec85454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3398988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake/codeup-data-science/natural-language-processing-exercises/acquire.py:11: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 11 of the file /Users/jake/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text) # soup\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>forward left hope</td>\n",
       "      <td>2005-08-16</td>\n",
       "      <td>By Sheri James</td>\n",
       "      <td>Friend off degree whether possible quickly dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however year stay</td>\n",
       "      <td>1979-09-09</td>\n",
       "      <td>By Christopher Steele</td>\n",
       "      <td>Police above small garden probably place. List...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instead plan senior</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>By Matthew Reed</td>\n",
       "      <td>Top word recent source quickly green organizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>act single price</td>\n",
       "      <td>1970-12-20</td>\n",
       "      <td>By Sandra Bennett</td>\n",
       "      <td>Item ten Congress. Of notice more business tax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tell kind station</td>\n",
       "      <td>2013-11-28</td>\n",
       "      <td>By Kevin Rios</td>\n",
       "      <td>Toward according discover study forget.\\nTend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>employee treatment body</td>\n",
       "      <td>2001-04-04</td>\n",
       "      <td>By Amber Johnson</td>\n",
       "      <td>Glass national though investment truth. Time s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>memory discuss enough</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>By Jill Lewis</td>\n",
       "      <td>More rate report hear newspaper price anyone. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>full guess clear</td>\n",
       "      <td>1999-11-21</td>\n",
       "      <td>By Matthew Green</td>\n",
       "      <td>Community be either direction middle chair. Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>everybody morning modern</td>\n",
       "      <td>1980-12-27</td>\n",
       "      <td>By Diane Dickson</td>\n",
       "      <td>Enough effort talk less area clear. Animal rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>local serve anyone</td>\n",
       "      <td>1979-05-22</td>\n",
       "      <td>By Victoria Larsen</td>\n",
       "      <td>Situation information relationship only relate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>change win they</td>\n",
       "      <td>1979-01-12</td>\n",
       "      <td>By Loretta Poole</td>\n",
       "      <td>Card ask successful development save serious r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>remain animal deal</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>By Jose Mendoza</td>\n",
       "      <td>Effort begin teach president. Front beat south...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title          date                  author  \\\n",
       "0          forward left hope   2005-08-16          By Sheri James    \n",
       "1          however year stay   1979-09-09   By Christopher Steele    \n",
       "2        instead plan senior   2017-06-20         By Matthew Reed    \n",
       "3           act single price   1970-12-20       By Sandra Bennett    \n",
       "4          tell kind station   2013-11-28           By Kevin Rios    \n",
       "5    employee treatment body   2001-04-04        By Amber Johnson    \n",
       "6      memory discuss enough   2012-01-06           By Jill Lewis    \n",
       "7           full guess clear   1999-11-21        By Matthew Green    \n",
       "8   everybody morning modern   1980-12-27        By Diane Dickson    \n",
       "9         local serve anyone   1979-05-22      By Victoria Larsen    \n",
       "10           change win they   1979-01-12        By Loretta Poole    \n",
       "11        remain animal deal   2017-04-09         By Jose Mendoza    \n",
       "\n",
       "                                             contents  \n",
       "0   Friend off degree whether possible quickly dow...  \n",
       "1   Police above small garden probably place. List...  \n",
       "2   Top word recent source quickly green organizat...  \n",
       "3   Item ten Congress. Of notice more business tax...  \n",
       "4   Toward according discover study forget.\\nTend ...  \n",
       "5   Glass national though investment truth. Time s...  \n",
       "6   More rate report hear newspaper price anyone. ...  \n",
       "7   Community be either direction middle chair. Bl...  \n",
       "8   Enough effort talk less area clear. Animal rea...  \n",
       "9   Situation information relationship only relate...  \n",
       "10  Card ask successful development save serious r...  \n",
       "11  Effort begin teach president. Front beat south...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquire.parse_gulde_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15be8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Zach's webscraping demo\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# def parse_news_article(article):\n",
    "#     title = article.h2.text\n",
    "#     date, author = article.select('.italic')[0].find_all('p')\n",
    "#     return {'title':title, 'date':date.text, 'author':author.text}\n",
    "# response = requests.get('https://web-scraping-demo.zgulde.net/news')\n",
    "# soup = BeautifulSoup(response.text)\n",
    "# articles = soup.select('.grid.gap-y-12 > div')\n",
    "# pd.DataFrame([parse_news_article(article) for article in articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b877010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # My adjustments to grab contents of articles\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# def parse_news_article(article):\n",
    "#     title = article.h2.text\n",
    "#     date, author, contents = article.select('.py-3')[0].find_all('p') # adjusted select, added contents\n",
    "#     return {'title':title, 'date':date.text, 'author':author.text, 'contents':contents.text} # added contents\n",
    "# response = requests.get('https://web-scraping-demo.zgulde.net/news')\n",
    "# soup = BeautifulSoup(response.text)\n",
    "# articles = soup.select('.grid.gap-y-12 > div')\n",
    "# pd.DataFrame([parse_news_article(article) for article in articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c49040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake/codeup-data-science/natural-language-processing-exercises/acquire.py:28: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 28 of the file /Users/jake/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text) # soup\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>quote</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gregory Bentley</td>\n",
       "      <td>\"Streamlined client-driven time-frame\"</td>\n",
       "      <td>wesleycrawford@yahoo.com</td>\n",
       "      <td>001-166-596-9599x264</td>\n",
       "      <td>234 Williams Mountain Lesliefurt, WV 79869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allen Silva</td>\n",
       "      <td>\"Operative value-added migration\"</td>\n",
       "      <td>bwilliams@hotmail.com</td>\n",
       "      <td>001-712-692-0415</td>\n",
       "      <td>51820 Lindsay Station Apt. 047 Riddlechester, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brett Mitchell</td>\n",
       "      <td>\"Seamless explicit synergy\"</td>\n",
       "      <td>ybush@lopez.biz</td>\n",
       "      <td>174.186.7194</td>\n",
       "      <td>14168 Bell Stravenue Suite 954 Barbaraville, L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander Salinas</td>\n",
       "      <td>\"Persevering intangible archive\"</td>\n",
       "      <td>adam87@wang.com</td>\n",
       "      <td>020.309.8671x6601</td>\n",
       "      <td>766 Justin Prairie Apt. 506 Port Maria, CO 48070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cynthia Caldwell</td>\n",
       "      <td>\"Intuitive coherent utilization\"</td>\n",
       "      <td>murrayashley@webster.com</td>\n",
       "      <td>050.521.5500x77853</td>\n",
       "      <td>00294 Hester Junction Bakermouth, NH 12534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jamie Edwards</td>\n",
       "      <td>\"Extended even-keeled pricing structure\"</td>\n",
       "      <td>rickbailey@hotmail.com</td>\n",
       "      <td>001-075-104-1129x379</td>\n",
       "      <td>801 Richard Ramp East Brendachester, WV 96912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mrs. Erin Terrell DDS</td>\n",
       "      <td>\"Synergized contextually-based product\"</td>\n",
       "      <td>angela41@gmail.com</td>\n",
       "      <td>(625)325-9870x2849</td>\n",
       "      <td>22678 Jonathan Walk North Justin, WY 91822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kerry Smith</td>\n",
       "      <td>\"Secured human-resource parallelism\"</td>\n",
       "      <td>maria73@yahoo.com</td>\n",
       "      <td>+1-083-026-9695x073</td>\n",
       "      <td>12629 Cunningham Overpass Hardinborough, MT 32938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Joshua Romero</td>\n",
       "      <td>\"Pre-emptive disintermediate matrices\"</td>\n",
       "      <td>qgilbert@gmail.com</td>\n",
       "      <td>(365)266-4411x753</td>\n",
       "      <td>422 Zachary Square Apt. 843 Joshuafort, IN 88124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Samantha Cooper</td>\n",
       "      <td>\"Synchronized systemic customer loyalty\"</td>\n",
       "      <td>eroberts@yahoo.com</td>\n",
       "      <td>+1-368-821-9453x43521</td>\n",
       "      <td>7758 Brandy Street Suite 806 Meyersborough, RI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                     quote  \\\n",
       "0        Gregory Bentley    \"Streamlined client-driven time-frame\"   \n",
       "1            Allen Silva         \"Operative value-added migration\"   \n",
       "2         Brett Mitchell               \"Seamless explicit synergy\"   \n",
       "3      Alexander Salinas          \"Persevering intangible archive\"   \n",
       "4       Cynthia Caldwell          \"Intuitive coherent utilization\"   \n",
       "5          Jamie Edwards  \"Extended even-keeled pricing structure\"   \n",
       "6  Mrs. Erin Terrell DDS   \"Synergized contextually-based product\"   \n",
       "7            Kerry Smith      \"Secured human-resource parallelism\"   \n",
       "8          Joshua Romero    \"Pre-emptive disintermediate matrices\"   \n",
       "9        Samantha Cooper  \"Synchronized systemic customer loyalty\"   \n",
       "\n",
       "                      email                  phone  \\\n",
       "0  wesleycrawford@yahoo.com   001-166-596-9599x264   \n",
       "1     bwilliams@hotmail.com       001-712-692-0415   \n",
       "2           ybush@lopez.biz           174.186.7194   \n",
       "3           adam87@wang.com      020.309.8671x6601   \n",
       "4  murrayashley@webster.com     050.521.5500x77853   \n",
       "5    rickbailey@hotmail.com   001-075-104-1129x379   \n",
       "6        angela41@gmail.com     (625)325-9870x2849   \n",
       "7         maria73@yahoo.com    +1-083-026-9695x073   \n",
       "8        qgilbert@gmail.com      (365)266-4411x753   \n",
       "9        eroberts@yahoo.com  +1-368-821-9453x43521   \n",
       "\n",
       "                                             address  \n",
       "0         234 Williams Mountain Lesliefurt, WV 79869  \n",
       "1  51820 Lindsay Station Apt. 047 Riddlechester, ...  \n",
       "2  14168 Bell Stravenue Suite 954 Barbaraville, L...  \n",
       "3   766 Justin Prairie Apt. 506 Port Maria, CO 48070  \n",
       "4         00294 Hester Junction Bakermouth, NH 12534  \n",
       "5      801 Richard Ramp East Brendachester, WV 96912  \n",
       "6         22678 Jonathan Walk North Justin, WY 91822  \n",
       "7  12629 Cunningham Overpass Hardinborough, MT 32938  \n",
       "8   422 Zachary Square Apt. 843 Joshuafort, IN 88124  \n",
       "9  7758 Brandy Street Suite 806 Meyersborough, RI...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquire.parse_gulde_people()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0b2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # people page\n",
    "# url = 'https://web-scraping-demo.zgulde.net/people'\n",
    "# response2 = requests.get(url)\n",
    "# soup2 = BeautifulSoup(response2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f452a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# people = soup2.find_all('div', {'class':'person'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae728a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_list = []\n",
    "# for person in people:\n",
    "#     name = person.h2.text\n",
    "#     quote, email, phone, address = person.find_all('p')\n",
    "#     quote, email, phone, address = quote.text.strip(), email.text, phone.text, address.text.strip()\n",
    "#     regexp = r'\\s{2,}'\n",
    "#     address = re.sub(regexp, ' ', address)\n",
    "#     person_dict = {'name':name, 'quote':quote, 'email':email, \n",
    "#                    'phone':phone, 'address':address}\n",
    "#     info_list.append(person_dict)\n",
    "\n",
    "# pd.DataFrame(info_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676a01a",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "## Codeup Blog Articles\n",
    "1. Scrape the article text from the following pages:\n",
    "- https://codeup.com/data-science/codeups-data-science-career-accelerator-is-here/\n",
    "- https://codeup.com/data-science/data-science-myths/\n",
    "- https://codeup.com/data-science/data-science-vs-data-analytics-whats-the-difference/\n",
    "- https://codeup.com/data-science/10-tips-to-crush-it-at-the-sa-tech-job-fair/\n",
    "- https://codeup.com/data-science/competitor-bootcamps-are-closing-is-the-model-in-danger/\n",
    "\n",
    "2. Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:\n",
    "- {'title': 'the title of the article', 'content': 'the full text content of the article'}\n",
    "- Plus any additional properties you think might be helpful.\n",
    "\n",
    "3. Bonus:\n",
    "- Scrape the text of all the articles linked on codeup's blog page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "185d07ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake/codeup-data-science/natural-language-processing-exercises/acquire.py:57: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 57 of the file /Users/jake/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text) # soup\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>Sep 30, 2018</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>Oct 31, 2018</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust Data Scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>Oct 17, 2018</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>By Dimitri Antoniou A week ago, Codeup launche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>Aug 14, 2018</td>\n",
       "      <td>Tips for Prospective Students</td>\n",
       "      <td>The third bi-annual San Antonio Tech Job Fair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>Aug 14, 2018</td>\n",
       "      <td>Codeup News</td>\n",
       "      <td>In recent news, DevBootcamp and The Iron Yar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title          date  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  Sep 30, 2018   \n",
       "1                                 Data Science Myths  Oct 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...  Oct 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair  Aug 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  Aug 14, 2018   \n",
       "\n",
       "                        category  \\\n",
       "0                   Data Science   \n",
       "1                   Data Science   \n",
       "2                   Data Science   \n",
       "3  Tips for Prospective Students   \n",
       "4                    Codeup News   \n",
       "\n",
       "                                             content  \n",
       "0  The rumors are true! The time has arrived. Cod...  \n",
       "1  By Dimitri Antoniou and Maggie Giust Data Scie...  \n",
       "2  By Dimitri Antoniou A week ago, Codeup launche...  \n",
       "3  The third bi-annual San Antonio Tech Job Fair ...  \n",
       "4    In recent news, DevBootcamp and The Iron Yar...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquire.get_blogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f0f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url1 = 'https://codeup.com/data-science/codeups-data-science-career-accelerator-is-here/'\n",
    "# url2 = 'https://codeup.com/data-science/data-science-myths/'\n",
    "# url3 = 'https://codeup.com/data-science/data-science-vs-data-analytics-whats-the-difference/'\n",
    "# url4 = 'https://codeup.com/data-science/10-tips-to-crush-it-at-the-sa-tech-job-fair/'\n",
    "# url5 = 'https://codeup.com/data-science/competitor-bootcamps-are-closing-is-the-model-in-danger/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a319e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = 'codeup ds germain'\n",
    "# response = requests.get(url5, headers={'User-Agent': agent})\n",
    "# soup5 = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "447f8f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title1 = soup5.select('.entry-title')[0].text\n",
    "# date1 = soup5.select('.published')[0].text\n",
    "# category1 = soup5.find_all('a', {'rel':'category tag'})[0].text\n",
    "# paragraphs = soup5.find_all('div', {'class':'et_pb_module et_pb_post_content et_pb_post_content_0_tb_body'})[0]\\\n",
    "# .find_all('p')\n",
    "# paragraph_list = []\n",
    "# for paragraph in paragraphs:\n",
    "#     paragraph_list.append(paragraph.text)\n",
    "# content1 = \" \".join(paragraph_list).replace('\\xa0', ' ')\n",
    "# pd.DataFrame([{'title':title1, 'date':date1, 'category':category1, 'content':content1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a23089f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(content1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1ff44",
   "metadata": {},
   "source": [
    "## News Articles\n",
    "We will now be scraping text data from inshorts, a website that provides a brief overview of many different topics.\n",
    "\n",
    "1. Write a function that scrapes the news articles for the following topics:\n",
    "- Business\n",
    "- Sports\n",
    "- Technology\n",
    "- Entertainment\n",
    "\n",
    "2. The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this example shape:\n",
    "- {'title': 'The article title', 'content': 'The article content', 'category': 'business'}\n",
    "\n",
    "3. Hints:\n",
    "- Start by inspecting the website in your browser. Figure out which elements will be useful.\n",
    "- Start by creating a function that handles a single article and produces a dictionary like the one above.\n",
    "- Next create a function that will find all the articles on a single page and call the function you created in the last step for every article on the page.\n",
    "- Now create a function that will use the previous two functions to scrape the articles from all the pages that you need, and do any additional processing that needs to be done.\n",
    "\n",
    "4. Bonus: cache the data\n",
    "- Write your code such that the acquired data is saved locally in some form or fashion. Your functions that retrieve the data should prefer to read the local data instead of having to make all the requests everytime the function is called. Include a boolean flag in the functions to allow the data to be acquired \"fresh\" from the actual sources (re-writing your local cache)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6a14e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake/codeup-data-science/natural-language-processing-exercises/acquire.py:84: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 84 of the file /Users/jake/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text) # soup\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India's Covaxin may get WHO approval in next 2...</td>\n",
       "      <td>04:48 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>A WHO technical advisory group which met on Tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elon Musk tweets 'Wild $T1mes' after Tesla hit...</td>\n",
       "      <td>02:30 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>Tesla CEO and the world's richest person Elon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I decided to support Doge as it felt like the ...</td>\n",
       "      <td>04:49 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>Tesla CEO and the world's richest person Elon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which companies have $1 trillion or more marke...</td>\n",
       "      <td>05:50 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>Tesla has become the latest company to surpass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many years did it take for various compan...</td>\n",
       "      <td>08:52 pm</td>\n",
       "      <td>Business</td>\n",
       "      <td>Tesla took 18 years to hit the $1-trillion m-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Jr NTR's fan injured in accident, actor helps ...</td>\n",
       "      <td>07:23 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>A fan of Telugu actor Jr NTR was injured in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Made Bollywood debut with 3-second role in Big...</td>\n",
       "      <td>07:28 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Actor Rajkummar Rao revealed that he made his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>It was 1 story for me: Kabir on not directing ...</td>\n",
       "      <td>10:18 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Filmmaker Kabir Khan spoke about why he didn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A film that will always remain part of me: Sun...</td>\n",
       "      <td>08:28 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>As 'Indian' completed 20 years of its release ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Constantly challenge myself to grow with each ...</td>\n",
       "      <td>11:15 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Filmmaker Ashwiny Iyer Tiwari has said that as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline publish_time  \\\n",
       "0   India's Covaxin may get WHO approval in next 2...     04:48 pm   \n",
       "1   Elon Musk tweets 'Wild $T1mes' after Tesla hit...     02:30 pm   \n",
       "2   I decided to support Doge as it felt like the ...     04:49 pm   \n",
       "3   Which companies have $1 trillion or more marke...     05:50 pm   \n",
       "4    How many years did it take for various compan...     08:52 pm   \n",
       "..                                                ...          ...   \n",
       "20  Jr NTR's fan injured in accident, actor helps ...     07:23 pm   \n",
       "21  Made Bollywood debut with 3-second role in Big...     07:28 pm   \n",
       "22  It was 1 story for me: Kabir on not directing ...     10:18 pm   \n",
       "23  A film that will always remain part of me: Sun...     08:28 pm   \n",
       "24  Constantly challenge myself to grow with each ...     11:15 pm   \n",
       "\n",
       "         category                                            content  \n",
       "0        Business  A WHO technical advisory group which met on Tu...  \n",
       "1        Business  Tesla CEO and the world's richest person Elon ...  \n",
       "2        Business  Tesla CEO and the world's richest person Elon ...  \n",
       "3        Business  Tesla has become the latest company to surpass...  \n",
       "4        Business  Tesla took 18 years to hit the $1-trillion m-c...  \n",
       "..            ...                                                ...  \n",
       "20  Entertainment  A fan of Telugu actor Jr NTR was injured in a ...  \n",
       "21  Entertainment  Actor Rajkummar Rao revealed that he made his ...  \n",
       "22  Entertainment  Filmmaker Kabir Khan spoke about why he didn't...  \n",
       "23  Entertainment  As 'Indian' completed 20 years of its release ...  \n",
       "24  Entertainment  Filmmaker Ashwiny Iyer Tiwari has said that as...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquire.get_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aed4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url1 = 'https://inshorts.com/en/read/business'\n",
    "# url2 = 'https://inshorts.com/en/read/sports'\n",
    "# url3 = 'https://inshorts.com/en/read/technology'\n",
    "# url4 = 'https://inshorts.com/en/read/entertainment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8743059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = 'codeup ds germain'\n",
    "# response = requests.get(url1, headers={'User-Agent': agent})\n",
    "# soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9793726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17f09f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # <span itemprop=\"headline\"> - headline\n",
    "# # <span class=\"time\" - publish time\n",
    "# # <li class=\"active-category selected\"> - category\n",
    "# # <div itemprop=\"articleBody\"> - article content\n",
    "# headline = soup.select('.news-card')[0].find_all('span', {'itemprop':'headline'})[0].text\n",
    "# publish_time = soup.select('.news-card')[0].find_all('span', {'class':'time'})[0].text\n",
    "# category = soup.find_all('li', {'class':'active-category selected'})[0].text\n",
    "# content = soup.select('.news-card')[0].find_all('div', {'itemprop':'articleBody'})[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "789588e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([{'headline':headline, 'publish_time':publish_time,\n",
    "#                'category':category, 'content':content}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d72bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_article(url):\n",
    "#     \"\"\" return dataframe of articles in inshorts category URL \"\"\"\n",
    "#     agent = 'codeup ds germain' # set agent\n",
    "#     response = requests.get(url, headers={'User-Agent': agent}) # query\n",
    "#     soup = BeautifulSoup(response.text) # soup\n",
    "#     category = soup.find_all('li', {'class':'active-category selected'})[0].text # get cat\n",
    "#     cards = soup.select('.news-card') # get raw cards\n",
    "#     card_dict_list = [] # create list of dicts for dataframe\n",
    "#     for card in cards: # iterate each card\n",
    "#         headline = card.find_all('span', {'itemprop':'headline'})[0].text # headline\n",
    "#         publish_time = card.find_all('span', {'class':'time'})[0].text # publish time\n",
    "#         content = card.find_all('div', {'itemprop':'articleBody'})[0].text.strip() # content\n",
    "#         card_dict = {'headline':headline, 'publish_time':publish_time,\n",
    "#                        'category':category, 'content':content} # create dict\n",
    "#         card_dict_list.append(card_dict) # push dict to list\n",
    "#     return pd.DataFrame(card_dict_list) # return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e7ab540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inshorts_urls():\n",
    "#     \"\"\" return list of inshorts URLs for exercise \"\"\"\n",
    "#     url1 = 'https://inshorts.com/en/read/business'\n",
    "#     url2 = 'https://inshorts.com/en/read/sports'\n",
    "#     url3 = 'https://inshorts.com/en/read/technology'\n",
    "#     url4 = 'https://inshorts.com/en/read/entertainment'\n",
    "#     return [url1, url2, url3, url4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d96ba119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_news():\n",
    "#     \"\"\" query, return dataframe of inshorts business, \n",
    "#         sports, tech, entertainment articles \"\"\"\n",
    "#     df = pd.DataFrame() # empty dataframe\n",
    "#     for url in inshorts_urls(): # read each url in list\n",
    "#         df = pd.concat([df, get_article(url)]) # add each dataframe of cards to df\n",
    "#     return df # return all urls' cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bc51cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b747e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
